{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ae10a2",
   "metadata": {},
   "source": [
    "# New Era Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e216a",
   "metadata": {},
   "source": [
    "#### Embedding : Are vectors or 1D arrays that use numbers to represent semantic properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca334f1a",
   "metadata": {},
   "source": [
    "Word embeddings are vital in NLP because they capture the relationship between words. Unless a model learns the relationship between words, it cannot perform more complex NLP tasks, such as text classification, well.\n",
    "\n",
    "One-hot encoding matrix would have high dimensions, it  would be a sparse matrix (mostly zero) and would suffer from the curse of dimensionality (e.g., we would need a lot of data to train a model that generalizes well because this matrix is both large and sparse, making parameter estimation more difficult). Also these are static vectors that don’t accurately capture the meanings of words. \n",
    "Today richer contextual embeddings exist, and they are much better than context-free embeddings like Word2vec and GloVe.\n",
    "\n",
    "Word embeddings trained by Word2Vec, GloVe, and fastText store **contextual and semantic information** for each word in a much lower dimensional space, unlike one-hot encoding. Words such as “queen” and “king” have vectors that are closer together in space, implying that there is some semantic relationship/similarity between the two.\n",
    "\n",
    "In 2013, pretrained word embeddings became popular with the rise of Word2Vec, the first of the major word embedding algorithms. \n",
    "\n",
    "Despite its successes, Word2Vec has shortcomings :\n",
    "\n",
    "1. It relies on a relatively small window-based model to learn the word embedding for a particular word. It does not consider the word in the context of the entire document. \n",
    "\n",
    "2. It does not consider subword information, which means that it cannot efficiently learn, for example, how a noun and an adjective that are derived from the same subword are related. For instance, “intelligent” and “intelligence” share the subword “intelligen” and are related as a result, sharing similar semantic information.\n",
    "\n",
    "3. Word2Vec cannot handle Out of Vocabulary (OOV) words; it can only vectorize words that it has seen in training.\n",
    "\n",
    "4. Word2Vec cannot disambiguate the context-specific semantic properties of words. For example, with Word2Vec, the word “bank” has the same word vector regardless of whether it appears in the financial setting or in the river setting.\n",
    "\n",
    "5. With fastText, the only major shortcoming is its inability to produce multiple vectors for each word depending on the context.\n",
    "\n",
    "As good as they are, word embeddings trained by **Word2Vec, GloVe, and fastText** are not context-aware. \n",
    "\n",
    "The large, pretrained language models based on the Transformer architecture, such as ELMo and BERT, that came onto the scene starting in 2018 changed this: they introduce context-aware word representations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd05d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621e7276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch\n",
    "\n",
    "from torchtext.legacy import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cce51ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adfbedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from simpletransformers.classification import ClassificationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8db6ad",
   "metadata": {},
   "source": [
    "### Bootstrapping Labelling using zer0-shot-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e91cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", device=0)\n",
    "candidate_labels=[\"Defect\", \"Software Research\", \"Software Upgrade\", \"Testing\", \"Software Development\", \"Software Maintenance\", \"Software Deployment\", \"Software Enhancement\", \"Software Integration\", \"New Feature\", \"Innovation\", \"R&D\", \"Continue Operation\", \"Other\", \"Technical Debt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39037403",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_keys = ['labels']\n",
    "def predict_0_shot_label(summary):\n",
    "    result = classifier(summary, candidate_labels)\n",
    "    temp = list(map(result.get, filter_keys))\n",
    "    zero_shot_label = temp[0][0]\n",
    "    return zero_shot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82834b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_label(label_text):\n",
    "    if label_text in ['New Feature', 'Innovation', 'R&D', 'Software Research', 'Software Development', 'Software Enhancement']:\n",
    "        return 2\n",
    "    elif label_text in ['Testing', 'Continue Operation', 'Technical Debt', 'Software Maintenance', 'Software Upgrade', 'Software Deployment', 'Software Integration']:\n",
    "        return 0\n",
    "    elif label_text in ['Defect']:\n",
    "        return 1   \n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f55f0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "IssuesDF['zero_shot_label'] = IssuesDF['summary'].apply(predict_0_shot_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c298fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IssuesDF['label'] = IssuesDF['zero_shot_label'].apply(translate_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbfae597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_id</th>\n",
       "      <th>summary</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53674</td>\n",
       "      <td>Sub-task Grow number of contributors from 37 t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53611</td>\n",
       "      <td>Task Own all aspects of customer success funct...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53609</td>\n",
       "      <td>Task Understand software development/testing p...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53494</td>\n",
       "      <td>Task Grow skills in troubleshooting customer's...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53260</td>\n",
       "      <td>Story Grow Kubernetes Knowledge</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>53254</td>\n",
       "      <td>Sub-task Define new policies and keep adding n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>53076</td>\n",
       "      <td>Story Grow Knowledge &amp; Skills</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>51923</td>\n",
       "      <td>Task Verify cluster creation with add-on EKS, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>47754</td>\n",
       "      <td>Task Implement log collection and aggregation ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>53817</td>\n",
       "      <td>Task PSP deprecation/Kyverno blog post</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  issue_id                                            summary  label\n",
       "0    53674  Sub-task Grow number of contributors from 37 t...      3\n",
       "1    53611  Task Own all aspects of customer success funct...      0\n",
       "2    53609  Task Understand software development/testing p...      2\n",
       "3    53494  Task Grow skills in troubleshooting customer's...      2\n",
       "4    53260                    Story Grow Kubernetes Knowledge      2\n",
       "5    53254  Sub-task Define new policies and keep adding n...      0\n",
       "6    53076                      Story Grow Knowledge & Skills      0\n",
       "7    51923  Task Verify cluster creation with add-on EKS, ...      0\n",
       "8    47754  Task Implement log collection and aggregation ...      2\n",
       "9    53817             Task PSP deprecation/Kyverno blog post      1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IssuesDF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aec00ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IssuesDF = IssuesDF.drop('zero_shot_label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "465fcd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import Field, Dataset, Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe798243",
   "metadata": {},
   "source": [
    "#### We are in need of a dataframe Dataset to use inout pytorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61644017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, fields: list):\n",
    "        super(DataFrameDataset, self).__init__(\n",
    "            [\n",
    "                Example.fromlist(list(r), fields) \n",
    "                for i, r in df.iterrows()\n",
    "            ], \n",
    "            fields\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2c067",
   "metadata": {},
   "source": [
    "#### Set up fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94ad5b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up fields\n",
    "ISSUE = data.Field()\n",
    "SUMMARY = data.Field(sequential=True, tokenize='basic_english', lower=True, include_lengths=True\n",
    "                     , batch_first=False)\n",
    "LABEL = data.LabelField()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc813d9",
   "metadata": {},
   "source": [
    "#### Getting the vectors and building the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f31f0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = DataFrameDataset(\n",
    "    df=IssuesDF, \n",
    "    fields=(\n",
    "        ('issue_id', ISSUE),\n",
    "        ('summary', SUMMARY),\n",
    "        ('label', LABEL)\n",
    "    )\n",
    ").split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e515b59",
   "metadata": {},
   "source": [
    "#### Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f313530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISSUE.build_vocab(train)\n",
    "SUMMARY.build_vocab(train\n",
    "                    #, vectors= 'glove.6B.100d'\n",
    "                    #, vectors='glove.42B.300d'\n",
    "                     , vectors='fasttext.simple.300d'\n",
    "                   )\n",
    "LABEL.build_vocab(train)\n",
    "EmbeddingSize = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ab74c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUMMARY.vocab.stoi['cloud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76183672",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vars(test[100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfbfceb",
   "metadata": {},
   "source": [
    "Dataset downloaded, tokenized, and vectorized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f058d",
   "metadata": {},
   "source": [
    "#### Make iterator for splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff66a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, test_iter = data.BucketIterator.splits((train, test),\n",
    "batch_sizes=(128,1024), device=dev\n",
    ", sort=False\n",
    ", repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d05bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(train_iter.dataset[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a80500",
   "metadata": {},
   "source": [
    "**We'll now build a model to process the Jira data word vectors.\n",
    "We will try different word embeddings and see their effect on performance on a simple dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e07e7",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40f151fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_classifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_size = EmbeddingSize, hidden_size = 512, num_layers = 3):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set up an embedding layer with the right dimensions, and copy the weights from the pretrained \n",
    "        # glove embeddings\n",
    "        vocab = SUMMARY.vocab\n",
    "        self.embed = torch.nn.Embedding(len(vocab), embedding_size).cuda()\n",
    "        self.embed.weight.data.copy_(vocab.vectors)\n",
    "\n",
    "        # Set up a standard PyTorch RNN sections with the right\n",
    "        # dimensions and a variable number of layers\n",
    "        self.rnn = torch.nn.RNN(embedding_size, hidden_size, num_layers)\n",
    "\n",
    "        # Add a two layer classification head with the right dimensions\n",
    "        # The final layer must output a single number\n",
    "        self.classificationLayer1 = torch.nn.Linear(hidden_size, 10)\n",
    "        self.classificationLayer2 = torch.nn.Linear(10, 1)\n",
    "\n",
    "\n",
    "    def forward(self, input, lengths=None):\n",
    "\n",
    "        embed_input = self.embed(input)\n",
    "        packed_emb = nn.utils.rnn.pack_padded_sequence(embed_input, lengths, batch_first=False, enforce_sorted=False)\n",
    "\n",
    "        output, hidden = self.rnn(packed_emb)\n",
    "        hidden = hidden[-1]\n",
    "        x = hidden.squeeze(0)\n",
    "        x = self.classificationLayer1(x)\n",
    "        x = self.classificationLayer2(x)\n",
    "\n",
    "        logits = x.view(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c073522d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_classifier(\n",
       "  (embed): Embedding(3450, 300)\n",
       "  (rnn): RNN(300, 256)\n",
       "  (classificationLayer1): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (classificationLayer2): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN_classifier(hidden_size=256, num_layers=1)\n",
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecbdc6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   8,   14,   22,  ...,   22,   14,   40],\n",
      "        [  17,   16,  125,  ..., 1593,   16, 1004],\n",
      "        [  46,  860,   50,  ..., 2476,   27, 1666],\n",
      "        ...,\n",
      "        [   1,    1,    1,  ...,    1, 1021,    1],\n",
      "        [   1,    1,    1,  ...,    1,   32,    1],\n",
      "        [   1,    1,    1,  ...,    1,    6,    1]], device='cuda:0')\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iter:\n",
    "    #print(batch)\n",
    "    x, x_len = batch.summary\n",
    "    x = x.to(dev)\n",
    "    x_len = torch.as_tensor(x_len, dtype=torch.int64, device='cpu')\n",
    "    print(x)\n",
    "    pred = model(x,x_len)\n",
    "    print(pred.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bb8a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.binary_cross_entropy_with_logits\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2925d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, test_data):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(test_data):\n",
    "            text, text_lengths = batch_data.summary\n",
    "            text = text.to(dev)\n",
    "            text_lengths = torch.as_tensor(text_lengths, dtype=torch.int64, device='cpu') \n",
    "            logits = model(text, text_lengths)\n",
    "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
    "            total += batch_data.label.size(0)\n",
    "            correct += (predicted_labels == batch_data.label.long()).sum()\n",
    "        return correct.float()/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e5b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dc12049",
   "metadata": {},
   "source": [
    "**With the change in the embedding vectors used when building the vocabulary from glove.42B.300d to fasttext.simple.300d, We see a 41% improvement in accuracy of classification from 56% to 79%.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b60cdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b56ecaa3d348c4933e4ebed351068c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d88fc0d5cf4d919e4f7c466ee626b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.35535428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7555ca69f7294135a4330c7120d72352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7842302\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac969583322d49cb8039216b302bc3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7927544\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf208167de548c0a5c458965f549a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79381996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7e5d550bb84e57b27449e4be39e245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78902507\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b30f3e0ff944d8989572c398db23250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79222167\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_iter):\n",
    "        x,x_lengths = batch.summary\n",
    "        x = x.to(dev)\n",
    "        x_lengths = torch.as_tensor(x_lengths, dtype=torch.int64, device='cpu')        \n",
    "        pred = model(x,x_lengths)\n",
    "\n",
    "        actual=batch.label.float()\n",
    "        loss = loss_func(pred,actual)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    if (epoch==5):\n",
    "        for g in opt.param_groups:\n",
    "            g['lr'] = 3e-3\n",
    "\n",
    "    print(\"Accuracy: \" + str(get_metrics(model, test_iter).cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116bc381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "584ee0ab",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb0b21",
   "metadata": {},
   "source": [
    "Embeddings have historically been generated with algorithms like Word2Vec, but with the advent of transfer learning, copying model weights allows you to copy embeddings as well, with no extra effort. \n",
    "\n",
    "Transformers use a feed-forward encoder-decoder with attention. This breakthrough in parallelization during training led to the advent of very large, pretrained language models and their emdeddings.\n",
    "\n",
    "With ELMo, it became possible to generate different word representations for the same word, such as “bank,” depending on the context it appeared in (financial bank versus river bank).\n",
    "\n",
    "Moreover, these word representations are character-based, like fastText word embeddings, which allows ELMo-based models to handle OOV tokens that weren’t seen during training. Unsurprisingly, adding ELMo’s contextualized word representations to existing NLP systems improved the state-of-the-art performance for every task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d6a620",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Data processing utilities for NLP, [Torchtext](https://pytorch.org/text/stable/index.html)\n",
    "\n",
    "\n",
    "[2] Applied Natural Language Processing in the Enterprise.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b39cb8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
